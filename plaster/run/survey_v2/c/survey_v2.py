import ctypes as c
import time
from contextlib import contextmanager, redirect_stderr, redirect_stdout
from io import StringIO

import numpy as np
import pandas as pd
from plaster.run.survey_v2.c.build import build
from plaster.tools.c_common import c_common_tools
from plaster.tools.c_common.c_common_tools import Hash, Tab
from plaster.tools.log.log import debug
from plaster.tools.utils import utils
from plumbum import local

lib_folder = local.path("/erisyon/plaster/plaster/run/survey_v2/c")

_lib = None


def load_lib():
    global _lib
    if _lib is not None:
        return _lib

    lib = c.CDLL(lib_folder / "_survey_v2.so")

    # C_COMMON
    lib.sanity_check.argtypes = []
    lib.sanity_check.restype = c.c_int

    # SURVEY_V2
    lib.context_start.argtypes = [
        c.POINTER(SurveyV2Context),
    ]
    lib.context_start.restype = c.c_int

    _lib = lib
    return lib


class SurveyV2Context(c_common_tools.FixupStructure):
    _fixup_fields = [
        ("dyemat", Tab, "Uint8"),
        ("dyepeps", Tab, "Uint64"),
        ("pep_i_to_dyepep_row_i", Tab, "Uint64"),
        ("dyt_i_to_n_reads", Tab, "Uint64"),
        ("dyt_i_to_mlpep_i", Tab, "Uint64"),
        ("output_pep_i_to_isolation_metric", Tab, "Float32"),
        ("output_pep_i_to_mic_pep_i", Tab, "Uint64"),
        ("next_pep_i", "Index"),
        ("n_threads", "Size"),
        ("n_flann_cores", "Size"),
        ("n_peps", "Size"),
        ("n_neighbors", "Size"),
        ("n_dyts", "Size"),
        ("n_dyt_cols", "Size"),
        ("distance_to_assign_an_isolated_pep", "Float32"),
        ("work_order_lock", "pthread_mutex_t *"),
        ("flann_params", "struct FLANNParameters *"),
        ("flann_index_id", "void *"),  # typedef void* flann_index_t;
        ("progress_fn", "ProgressFn"),
    ]


def init():
    """
    Must be called before anything else in this module
    """

    SurveyV2Context.struct_fixup()

    with local.cwd(lib_folder):
        fp = StringIO()
        with redirect_stdout(fp):
            print(f"// This file was code-generated by survey_v2.c.survey_v2.init")
            print()
            print("#ifndef SIM_V2_H")
            print("#define SIM_V2_H")
            print()
            print('#include "stdint.h"')
            print('#include "c_common.h"')
            print()
            SurveyV2Context.struct_emit_header(fp)
            print("#endif")

        header_file_path = "./_survey_v2.h"
        existing_h = utils.load(header_file_path, return_on_non_existing="")

        if existing_h != fp.getvalue():
            utils.save(header_file_path, fp.getvalue())

        build(
            dst_folder=lib_folder,
            c_common_folder="/erisyon/plaster/plaster/tools/c_common",
            flann_include_folder="/flann/src/cpp/flann",
            flann_lib_folder="/flann/lib",
        )
        lib = c.CDLL("./_survey_v2.so")


def _assert_array_contiguous(arr, dtype):
    assert isinstance(arr, np.ndarray)
    assert arr.dtype == dtype, f"{arr.dtype} {dtype}"
    assert arr.flags["C_CONTIGUOUS"]


global_progress_callback = None


@c.CFUNCTYPE(c.c_voidp, c.c_int, c.c_int, c.c_int)
def progress_fn(complete, total, retry):
    if global_progress_callback is not None:
        global_progress_callback(complete, total, retry)


IsolationNPType = np.float32


# Wrapper for survey that prepares buffers for csurvey
def survey(
    n_peps, dyemat, dyepeps, n_threads=1, progress=None,
):
    lib = load_lib()

    # Vars
    n_dyts = dyemat.shape[0]

    pep_column_in_dyepeps = 1

    assert lib.sanity_check() == 0

    global global_progress_callback
    global_progress_callback = progress

    # SETUP the dyemat table
    _assert_array_contiguous(dyemat, np.uint8)

    # BUILD a LUT from dyt_i to most-likely peptide i (mlpep_i)
    # The dyepep_df can have missing pep_i (there are peptides that have no dyt_i)
    # But all dyt have peps.
    dyepep_df = pd.DataFrame(dyepeps, columns=["dyt_i", "pep_i", "n_reads"])

    # EXTRACT the row in each dyt_i group that has the most reads; this is the Most-Likely-Pep
    dyt_i_to_mlpep_i = dyepep_df.loc[
        dyepep_df.groupby(["dyt_i"])["n_reads"].idxmax()
    ].reset_index()
    assert np.unique(dyt_i_to_mlpep_i.dyt_i).tolist() == list(range(n_dyts))

    dyt_i_to_mlpep_i = dyt_i_to_mlpep_i.pep_i.values
    assert (len(dyt_i_to_mlpep_i)) == n_dyts
    dyt_i_to_mlpep_i = np.ascontiguousarray(dyt_i_to_mlpep_i, dtype=np.uint64)
    _assert_array_contiguous(dyt_i_to_mlpep_i, np.uint64)

    # FILL-in missing pep_i from the dataframe
    # This is tricky because there can be duplicate "pep_i" rows and the simple reindex
    # answer from SO doesn't work in that case so we need to make a list of the missing rows
    new_index = pd.Index(np.arange(n_peps), name="pep_i")

    # Drop duplicates from dyepep_df so that the reindex can work...
    missing = dyepep_df.drop_duplicates("pep_i").set_index("pep_i").reindex(new_index)

    # Now missing has all rows, and the "new" rows (ie those that were missing in dyepep_df)
    # have NaNs in their dyt_i fields, so select those out.
    missing = missing[np.isnan(missing.dyt_i)].reset_index()

    # Now we can merge those missing rows into the dyepep_df
    dyepep_df = pd.merge(
        dyepep_df, missing, on="pep_i", how="outer", suffixes=["", "_dropme"]
    ).drop(columns=["dyt_i_dropme", "n_reads_dropme"])
    dyepep_df = dyepep_df.sort_values(["pep_i", "dyt_i"]).reset_index(drop=True)
    dyepep_df = dyepep_df.fillna(0).astype(np.uint64)

    # SETUP the dyt_i_to_n_reads
    assert np.unique(dyepep_df.dyt_i).tolist() == list(range(n_dyts))
    dyt_i_to_n_reads = np.ascontiguousarray(
        dyepep_df.groupby("dyt_i").sum().reset_index().n_reads.values, dtype=np.uint64
    )

    # SETUP the dyepeps tab, sorting by pep_i.  All pep_i must occur in this.
    assert np.unique(dyepep_df.pep_i).tolist() == list(range(n_peps))
    dyepeps = np.ascontiguousarray(dyepep_df.values, dtype=np.uint64)
    _assert_array_contiguous(dyepeps, np.uint64)

    _pep_i_to_dyepep_row_i = np.unique(dyepep_df.pep_i, return_index=1)[1].astype(
        np.uint64
    )
    pep_i_to_dyepep_row_i = np.zeros((n_peps + 1), dtype=np.uint64)
    pep_i_to_dyepep_row_i[0:n_peps] = _pep_i_to_dyepep_row_i
    pep_i_to_dyepep_row_i[n_peps] = dyepeps.shape[0]
    _assert_array_contiguous(pep_i_to_dyepep_row_i, np.uint64)

    # SANITY CHECK
    # print(", ".join([f"{i}" for i in pep_i_to_dyepep_row_i.tolist()]))
    assert np.all(np.diff(pep_i_to_dyepep_row_i) >= 0), "bad pep_i_to_dyepep_row_i"

    pep_i_to_isolation_metric = np.zeros((n_peps,), dtype=IsolationNPType)
    _assert_array_contiguous(pep_i_to_isolation_metric, IsolationNPType)
    pep_i_to_mic_pep_i = np.zeros((n_peps,), dtype=np.uint64)
    _assert_array_contiguous(pep_i_to_mic_pep_i, np.uint64)

    ctx = SurveyV2Context(
        dyemat=Tab.from_mat(dyemat, expected_dtype=np.uint8),
        dyepeps=Tab.from_mat(dyepeps, expected_dtype=np.uint64),
        pep_i_to_dyepep_row_i=Tab.from_mat(
            pep_i_to_dyepep_row_i, expected_dtype=np.uint64
        ),
        dyt_i_to_n_reads=Tab.from_mat(dyt_i_to_n_reads, expected_dtype=np.uint64),
        dyt_i_to_mlpep_i=Tab.from_mat(dyt_i_to_mlpep_i, expected_dtype=np.uint64),
        output_pep_i_to_isolation_metric=Tab.from_mat(
            pep_i_to_isolation_metric, expected_dtype=IsolationNPType
        ),
        output_pep_i_to_mic_pep_i=Tab.from_mat(
            pep_i_to_mic_pep_i, expected_dtype=np.uint64
        ),
        n_threads=1,
        n_flann_cores=n_threads,
        n_peps=n_peps,
        n_neighbors=10,
        n_dyts=n_dyts,
        n_dyt_cols=dyemat.shape[1],
        distance_to_assign_an_isolated_pep=10,  # TODO: Find this by sampling.
        progress_fn=progress_fn,
    )

    lib.context_start(ctx)

    return pep_i_to_mic_pep_i, pep_i_to_isolation_metric
